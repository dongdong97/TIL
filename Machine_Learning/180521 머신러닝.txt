챕터3 

p5
선을 나누기 가장 가까운벡터값에의해 선을 만드는데
각 벡터가 여백이 최대가 가지는 부분에 선을 긋는 것이 최선이다. 즉 둘이 일정한 거리를 가진 선을 가지는 것이 가장 좋다.

P6
2번을 하면 오차가 생길 가능성이 높다. 를 보여주는 것이다.
따라서 여백이 최대가 되게 만드는 것이 좋다.

P10
2차원이 아닌 3차원에서도 svm을 적용한다.

P14  구하는 식을 자세히 설명한 페이지
우리가 구현 x 턴서가 알아서 구현해준다.

거리를 구할때 벡터 값이 들어가기에 s'v'm 인거다.


xor같이 한번에 선으로 나타내기 애매할때는 차원을 증가시켜 하면 알 수 있다.

2차원을 3차원으로. 1차원을 2차원으로. 등등 다양하게 있다. 원형으로 측정되는 값을 다른 차원으로

P31 자주 쓰이는 커널 함수들. -> 커널 = 차원을 1차원 증겨시켜주는 것들

svm 끝

Normalization 정규화 시작. P34

다양한 값들을 0-1 사이 표준편차로 데이터로 만들어야 우리가 봐도 알아보기 쉬워서

거리가 가장 가까운 training data k개를 선택해 분류한다.

표준편차를 이용해서 이루어 내는 것 standardizaton 으로 만들어둔다.

Ex) p39 

정규화 된 데이터를 표준편차로 정규화 시켜 데이터 비교

P42 그냥 데이터를 정규화 해서 나온 값 보기
정규화를 하는 이유 값의 끝을 한번에 인식하기 어려워서 0-1로 줄여버려 한번에 많은지 적은지를 인식하는 것이다.

CV TEST
자신의 데이터중 몇개를 테스트로 할 것인데 오차가 매번 다르게 나올 수 있기에 다 해보는 경우를 만들어보기위해

오차범위를 줄이기 위해 k번으로 나눠서 하는 것이다. 

4장
질적분류
수치가 없어서 내가 따져야한다.

분류기를 만들어야한다. 결정 틀을 만들어야 한다. 스무고개 같은 틀 만들기
스무고개를 잘 만들어야 한다. 잘못하면 무수히 많은 차원을 만들게 되기때문

2가지로 나눠질수도 있고 다양하게 나눠질 수도 있다.
그래도 보통 이진트리 형식이다.
가장 중요***. 왼쪽 오른쪽 트리 상관관계가 0 에 가까워야 이상적이다. 고양이 사자 이런거 말고 강아지 새 이런식으로 되어야 좋다.
위에 있으면 많이 나눠주기 아래로 갈수록 자세히 만들어주기

불순도 는 0이 될수록 좋은것이다. 0이 될수록 원하던 분류에 가까워 지는 것이다. 
불순도가0이되려면. 질문이 1개밖에 없을때
상관관계를 구할때 엔트로피에 의해 불순도 기준 가능. 즉 불순도 = 엔트로피

불순도는 위치마다 값의 편차가 클 순 있어도 불순도 감소량이 클수록 좋은 것이다. 
후보가 될 수 있는 기준들을 다 불순도로 구해 값이 가장 많이 줄어든것을 사용

다양한 경우의 수를 하면서 내가 정의 한 기준에 만족하면 더이상 비교하지 않고 멈춘다.


결정트리 장점 1. 계량 값을 갖는 것 뿐 아닌 비계량이 있어도 할 수 있다. 2. 분류결과 해석 가능( 가시적이라 트리를 보고 해석 가능)
3. TF로 나타나기에 한번에 보기 편하다.

SVM / 정규화 / decisionTree 를 배웠당!



pip install scikit-learn